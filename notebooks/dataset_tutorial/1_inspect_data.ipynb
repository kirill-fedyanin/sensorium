{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to the Neuronal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### This notebook will show these items:\n",
    "- how to load the data into PyTorch DataLoaders\n",
    "- what is contained in the data\n",
    " - images\n",
    " - neuronal responses\n",
    " - behavioral variables\n",
    " - anatomical coordinates of each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nnfabrik\n",
    "from nnfabrik.builder import get_data, get_model, get_trainer\n",
    "import sensorium\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lipstick import GifMaker\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Instantiate PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filenames = ['../data/static23964-4-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip', ]\n",
    "\n",
    "dataset_fn = 'sensorium.datasets.static_loaders'\n",
    "dataset_config = {'paths': filenames,\n",
    "                 'normalize': True,\n",
    "                 'include_behavior': True,\n",
    "                 'include_eye_position': True,\n",
    "                 'batch_size': 128,\n",
    "                 'exclude': None,\n",
    "                 'file_tree': True,\n",
    "                 'scale': 1,\n",
    "                 'add_behavior_as_channels': False,\n",
    "                 }\n",
    "\n",
    "dataloaders = get_data(dataset_fn, dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Show Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The neuronal data is supplied as a PyTorch DataLoader, already split into the training and validation images. The test set is also available, containing 100 images with 10 repeats each. The two recordings provided for both competition tracks will also have a \"test\" tier, but they will not contain the ground truth neuronal responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('train',\n",
       "              OrderedDict([('23964-4-22',\n",
       "                            <torch.utils.data.dataloader.DataLoader at 0x7f3573f69f40>)])),\n",
       "             ('validation',\n",
       "              OrderedDict([('23964-4-22',\n",
       "                            <torch.utils.data.dataloader.DataLoader at 0x7f3573f5dd90>)])),\n",
       "             ('test',\n",
       "              OrderedDict([('23964-4-22',\n",
       "                            <torch.utils.data.dataloader.DataLoader at 0x7f3573f5d6a0>)])),\n",
       "             ('final_test',\n",
       "              OrderedDict([('23964-4-22',\n",
       "                            <torch.utils.data.dataloader.DataLoader at 0x7f3573f5d1c0>)]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Show Content of a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tier = 'train'\n",
    "dataset_name = '23964-4-22'\n",
    "\n",
    "for batch in dataloaders[tier][dataset_name]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('images', 'responses', 'behavior', 'pupil_center')\n"
     ]
    }
   ],
   "source": [
    "# each batch is a NameTuple, containing the images, neuronal responses, and the behavioral variables\n",
    "print(batch._fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images, torch.Size([10, 1, 144, 256])\n",
      "responses, torch.Size([10, 8098])\n",
      "behavior, torch.Size([10, 3])\n",
      "pupil_center, torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the content of a batch\n",
    "for i, field in enumerate(batch._fields):\n",
    "    print(f\"{field}, {batch[i].shape}\")\n",
    "\n",
    "# what will get printed:\n",
    "# images: N images, channels, height, width\n",
    "# responses: N images, N neurons\n",
    "# behavior: N images, N=3 behaviors (pupil size, instantaneous change of pupil size, locomotion speed)\n",
    "# pupil_center: N images, N=2 eye position traces: horizontal and vertical eye position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get all Images, Responses, and Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images, responses = [], []\n",
    "behaviors, eye_positions = [], []\n",
    "\n",
    "for batch in dataloaders[tier][dataset_name]:\n",
    "    images.append(batch.images.cpu().data.numpy())\n",
    "    responses.append(batch.responses.squeeze().cpu().data.numpy())\n",
    "    behaviors.append(batch.behavior.squeeze().cpu().numpy())\n",
    "    eye_positions.append(batch.pupil_center.squeeze().cpu().numpy())\n",
    "    \n",
    "images = np.vstack(images)\n",
    "responses = np.vstack(responses)\n",
    "all_behaviors = np.hstack([np.vstack(behaviors), np.vstack(eye_positions), ])\n",
    "\n",
    "print('The \\\"{}\\\" set of dataset \\\"{}\\\" contains the responses of {} neurons to {} images'.format(tier, dataset_name, responses.shape[1], responses.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "responses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Recreate Plots from Fig. 2 of the White Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Raster Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig,  ax = plt.subplots(1,1, figsize=(10,10))\n",
    "\n",
    "n_neurons = 200\n",
    "n_images = 300\n",
    "ax.imshow(responses[:n_images, :n_neurons], cmap=\"gray\",vmin=0, vmax=5)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "ax.set_xlabel(\"Natural Images\", fontsize=20, )\n",
    "ax.set_ylabel(\"Associated neuronal Response\", fontsize=20, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_behaviors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Example Behavior Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,2, figsize=(14,12))\n",
    "\n",
    "behaviors = [\"Pupil size\", \n",
    "            \"Change of pupil size\", \n",
    "            \"Locomotion speed\",\n",
    "            \"horizontal eye position\",\n",
    "            \"vertical eye position\"]\n",
    "colormaps = [\"YlGn\", \"gray\", \"RdPu\", \"plasma\", \"plasma\"]\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    for behavior in range(5):\n",
    "        if i == 0:\n",
    "            axs[behavior, i].imshow(np.tile(all_behaviors[:200, behavior], (40,1)), cmap=colormaps[behavior])\n",
    "            axs[behavior, i].set_xlabel(\"Trial number\", fontsize=12)\n",
    "            axs[behavior, i].set_title(behaviors[behavior], fontsize=16)\n",
    "        else:\n",
    "            axs[behavior, i].plot(all_behaviors[:200, behavior], 'k-')\n",
    "            axs[behavior, i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Show Individual Images and the associated response of all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show some example images and the neural responses\n",
    "n_images = 5\n",
    "max_response = responses[:n_images].max()\n",
    "\n",
    "for i in range(n_images):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15,4))\n",
    "    axs[0].imshow(images[i][0], cmap=\"gray\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].plot(responses[i])\n",
    "    axs[1].set_xlabel('Neurons', fontsize=16)\n",
    "    axs[1].set_ylabel('Responses', fontsize=16)\n",
    "    axs[1].set_ylim([0, max_response])\n",
    "    sns.despine(trim=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Show Anatomical Coordinates of all Neurons in the recorded Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### compute time ~1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_keys = list(dataloaders[\"train\"].keys())\n",
    "\n",
    "with GifMaker(f\"scan_fields_evaluation_mice.gif\", fps=24) as g:\n",
    "\n",
    "    for trial, azimuth in enumerate(np.linspace(0, 360, 180)):\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        \n",
    "        for i, data_key in enumerate(data_keys):\n",
    "            ax = fig.add_subplot(1, 1, i+1, projection='3d')\n",
    "            cell_motor_coordinates = (dataloaders[\"train\"][data_key].dataset.neurons.cell_motor_coordinates)\n",
    "            ax.scatter3D(*cell_motor_coordinates.T, c=\"k\", s=15, edgecolor=\"k\", alpha=0.5, cmap=\"magma\")\n",
    "            ax.view_init(45, azimuth)\n",
    "            \n",
    "            # bring all coordinates on the same scale\n",
    "            ax.set_zlim([100,340])\n",
    "            ax.set_title(f\"Anatomical positions of neurons in Recording {data_key}\", fontsize=16)\n",
    "            ax.set_xlabel('x coordinate', fontsize=16)\n",
    "            ax.set_ylabel('y coordinate', fontsize=16)\n",
    "            ax.set_zlabel('z coordinate', fontsize=16)\n",
    "        g.add(fig)\n",
    "g.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}